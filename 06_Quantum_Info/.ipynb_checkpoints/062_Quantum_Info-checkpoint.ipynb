{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3710bfdb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../figuras/logos/logo_usc.jpg\" align=right width='80px'/>\n",
    "<br>\n",
    "\n",
    "\n",
    "<table width=\"100%\">\n",
    "<td style=\"font-size:40px;font-style:italic;text-align:right;background-color:rgba(0, 220, 170,0.7)\">\n",
    "Quantum Information Theory\n",
    "</td></table>\n",
    "\n",
    "\n",
    "\n",
    "$ \\newcommand{\\bra}[1]{\\langle #1|} $\n",
    "$ \\newcommand{\\ket}[1]{|#1\\rangle} $\n",
    "$ \\newcommand{\\braket}[2]{\\langle #1|#2\\rangle} $\n",
    "$ \\newcommand{\\ketbra}[2]{| #1\\rangle \\langle #2|} $\n",
    "$ \\newcommand{\\tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\Tr}{{\\rm Tr}\\,} $\n",
    "$ \\newcommand{\\i}{{\\color{blue} i}} $ \n",
    "$ \\newcommand{\\Hil}{{\\cal H}} $\n",
    "$ \\newcommand{\\V}{{\\cal V}} $\n",
    "$ \\newcommand{\\Lin}{\\hbox{Lin}}$\n",
    "$ \\newcommand{\\Xn}{X^{\\! n}}$\n",
    "$ \\newcommand{\\xn}{{\\bf x}}$\n",
    "$ \\newcommand{\\bxn}{\\bar{\\bf x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3daa108",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<a id='top'></a>\n",
    "\n",
    " \n",
    " - [Quantum Information](#quant_info)  \n",
    "     - [Von Neumann Entropy](#vonNeu)\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd75c76b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='quant_info'></a>\n",
    "\n",
    "# Elements of Quantum Information\n",
    "[<<<](#top)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eff98b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Suppose a classical random source generates letters from an alphabet $X = \\{x_a, p_a\\}$ with probability $p_a = p(x_a)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d5bb9",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The *prior uncertainty* (information) is given by the Shannon entropy:\n",
    "\n",
    "$$\n",
    "H(X) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f0cda",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To transmit a message using a *quantum channel*, we prepare states $x_i \\to \\ket{\\psi_i}$ with $i$ devices and send them successively.\n",
    "\n",
    "In this way, we have created a *quantum signal source*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b6310",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "From the receiver's point of view, this is an incoherent statistical mixture of states $X = \\{\\ket{\\psi_a}, p_a\\}$, which are received with probability $p_a = p(\\ket{\\psi_a})$.\n",
    "\n",
    "To decode the message, the receiver must guess which states compose the received state by performing measurements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b483de9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The <b>density operator</b> is the mathematical object that characterizes the statistical mixture that is received  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\rho = \\sum_{a} p_a \\ket{\\psi_a}\\bra{\\psi_a}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f67f85",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that:\n",
    "\n",
    "- We have not required $\\ket{\\psi_a}$ to be a set of orthogonal vectors. In general, they will not be.\n",
    "<br>\n",
    "<br>\n",
    "- The number of vectors and letters $a = 1, 2, \\dots$ may be greater or smaller than the dimension of the Hilbert space of the quantum system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4e360",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\rho$ is Hermitian, we can always write it in its *spectral representation*:\n",
    "\n",
    "$$\n",
    "\\rho = \\sum_{i=1}^N \\lambda_i \\ket{\\lambda_i}\\bra{\\lambda_i}\n",
    "$$\n",
    "\n",
    "where $\\lambda_i$ are the eigenvalues and $\\ket{\\lambda_i}$ are the corresponding eigenvectors, which form an orthonormal basis: $\\braket{\\lambda_i}{\\lambda_j} = \\delta_{ij}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20903b7f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This representation refers to a *hypothetical device* associated with projective measurements $\\{P_i = \\ketbra{\\lambda_i}{\\lambda_i}\\}$.\n",
    "\n",
    "We refer to the quantum random variable $\\hat C = \\{\\ket{\\lambda_i}, \\lambda_i\\}$ as the *canonical ensemble*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7977ae2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='vonNeu'></a>\n",
    "\n",
    "## Von Neumann Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de20cf",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The described procedure presents us with two ensembles:\n",
    "\n",
    "- the *original* one, associated with the preparation: $X = \\{x_a, p_a\\} \\to \\{\\ket{\\psi_a}, p_a\\}$\n",
    "<br>\n",
    "<br>\n",
    "- the *canonical* one, associated with the diagonalization of $\\rho$: $C = \\{\\ket{\\lambda_i}, \\lambda_i\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70240b49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Each ensemble has an associated Shannon entropy:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "H(X) &=& -\\sum_a p_a \\log p_a \\\\\n",
    "H(C) &=& -\\sum_{i=1}^N \\lambda_i \\log \\lambda_i\n",
    "\\end{eqnarray}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e4215",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The key to the second expression is that, by definition, it is equivalent to the following:  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "H(C) = -\\Tr (\\rho \\log \\rho)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "The advantage of writing it this way is that it is *basis-independent*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d9554",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> <i>von Neumann entropy</i>  \n",
    "<br>    \n",
    "$$\n",
    "S(\\rho) = -\\Tr (\\rho\\log \\rho)\n",
    "$$  \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb4326",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Note that, written in this way, $S(\\rho)$:\n",
    "\n",
    "- does not refer to *any specific* basis of states.  \n",
    "<br>\n",
    "<br>\n",
    "- is uniquely determined for each state $\\rho$  \n",
    "<br>\n",
    "<br>\n",
    "- also does not depend on the preparation procedure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a266f4d1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In short: *we can assign a von Neumann entropy to any density operator* $\\rho$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5924a4d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise:</b>  \n",
    "Write a function `S_entropy(rho)` that returns the von Neumann entropy associated with a state $\\rho$,  \n",
    "expressed as a matrix in the canonical basis.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac593849",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of the von Neumann Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0375bb4a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Bounds**:  \n",
    "<br>Let $N$ be the <u>dimension of $\\Hil$</u>. The von Neumann entropy is bounded by  \n",
    "<br>\n",
    "<br>$$0 \\leq S(\\rho) \\leq \\log N$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f1785",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- In a <i>pure state</i>, the von Neumann entropy is zero  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S(\\rho) = 0 ~~~\\Longleftrightarrow ~~~\\rho^2 = \\rho\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9275dfa1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In a <i>maximally mixed state</i>, the entropy of the state is maximal  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S(\\rho) = \\log N ~~~\\Longleftrightarrow ~~~\\rho = \\frac{1}{N} I\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348cacb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Concavity**:  \n",
    "<br>\n",
    "\n",
    "$S(\\rho)$ is a concave function of its argument $\\rho$. For any straight line interpolating between $\\rho_1$ and $\\rho_2$:  \n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S\\left(\\rule{0mm}{4mm}\\lambda \\rho_1 + (1-\\lambda) \\rho_2 \\right) \\geq \\lambda S(\\rho_1) + (1-\\lambda) S(\\rho_2)\n",
    "$$\n",
    "<br>\n",
    "where $\\lambda \\in (0,1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02575121",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b>  \n",
    "The concavity of $S$ generalizes to linear combinations. Let $\\rho = \\sum_{i=1}^r p_i \\rho_i$, where $~\\sum_{i=1}^r p_i = 1$  \n",
    "<br>    \n",
    "<br>    \n",
    "$$\n",
    "S(\\rho) \\geq \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9709d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The proof will be given later by means of the subadditivity property.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f299ed4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- **Invariance**:  \n",
    "<br> The von Neumann entropy is invariant under *unitary transformations*:\n",
    "\n",
    "$$ \n",
    "S(\\rho) = S(U^\\dagger \\rho U)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb02b95",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In particular, this implies that the von Neumann entropy of an isolated system is constant in time  \n",
    "\n",
    "$$\n",
    "S(\\rho(t)) = S(U(t)\\rho(0) U(t)^\\dagger) = S(\\rho(0))\n",
    "$$\n",
    "That is,\n",
    "\n",
    "$$\n",
    "\\frac{dS(t)}{dt} = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de64a64",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='entrop_rel'></a>\n",
    "## Relative Entropy\n",
    "\n",
    "We define the relative entropy by formal analogy with the classical case. Let $\\rho$ and $\\sigma$ be two quantum states. The relative entropy is a measure of distance that vanishes when they are equal:\n",
    "\n",
    "$$\n",
    "S(\\rho \\| \\sigma) = \\Tr \\rho(\\log\\rho - \\log \\sigma)\n",
    "$$\n",
    "<br>\n",
    "\n",
    "Gibbs' inequality for $H(X\\|Y)$ has a parallel result for $S(\\rho \\| \\sigma)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa7e1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem</b> <i>(Klein's inequality)</i>  \n",
    "<br>\n",
    "Relative entropy is non-negative:  \n",
    "$$\n",
    "S(\\rho \\| \\sigma) \\geq 0\n",
    "$$  \n",
    "<br>\n",
    "and it vanishes if and only if $\\rho = \\sigma$.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb01cc0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='ent_for_med'></a>\n",
    "## Preparation and Measurement Entropies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7ffde5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Preparation Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52a459c",
   "metadata": {},
   "source": [
    "There are infinitely many ensembles  \n",
    "$X = \\{\\ket{\\psi_a}, p_a\\},\\quad \\tilde X = \\{\\ket{\\tilde \\psi_i}, \\tilde p_i\\}, \\dots$  \n",
    "that are described by the same density operator:\n",
    "\n",
    "$$\n",
    "\\rho ~=~ \\sum_{a=1}^r p_a \\ket{\\psi_a}\\bra{\\psi_a} ~=~ \\sum_{i=1}^s \\tilde p_i \\ket{\\tilde\\psi_i}\\bra{\\tilde\\psi_i} ~=~ \\dots\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19769b57",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each ensemble has an associated Shannon entropy $H(X), H(\\tilde X), \\dots$ which may differ.  \n",
    "<br>\n",
    "\n",
    "- However, the von Neumann entropy $S(\\rho)$ is the same for all of them because it depends only on $\\rho$.\n",
    "`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc3ffbb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "    <b>Definition:</b> <i>(Preparation entropy)</i>  \n",
    "<br>    \n",
    "For each ensemble $X = \\{\\ket{\\psi_a}, p_a\\}$ that prepares a state  \n",
    "$\\rho = \\sum_a p_a\\ket{\\psi_a}\\bra{\\psi_a}$,  \n",
    "we define the <i>preparation entropy</i> as the difference  \n",
    "<br>  \n",
    "<br>    \n",
    "$$\\Delta(X,\\rho) = H(X) - S(\\rho)$$\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb583d3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\", text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> The preparation entropy is non-negative, $\\Delta(X,\\rho) \\geq 0$, that is:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    "S(\\rho ) ~~\\leq ~~ H(X)\n",
    "\\\\\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "The inequality is saturated for a preparation $X$ in which the states $\\{\\ket{\\psi_a}\\}$ are orthogonal.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3f95d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The proof is lengthy and will not be presented here.  \n",
    "The result is plausible because $H(X) \\leq \\log(r)$, where $r$ is the number of *letters* in the ensemble $\\ket{\\psi_a}, \\, a = 1,\\dots,r$, which is unbounded, while $S(\\rho)\\leq \\log N$ is bounded by the dimension of the Hilbert space $\\Hil$.  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "- On the other hand, if we require the states to be orthogonal, then $r \\leq N$. We can complete them to form a basis $\\{\\ket{\\psi_a}\\}, \\, a = 1,\\dots,N$. Since $S(\\rho)$ is invariant under unitary transformations, it equals the expression written in the eigenbasis $\\{\\ket{\\lambda_a}\\}$, that is, $H$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c40550",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If the source states $X = \\{\\ket{\\psi_a}, p_a\\}$ are not orthogonal, then $S < H$. But the  \n",
    "$\\{\\ket{\\psi_a}\\}$ cannot be distinguished $\\Rightarrow$ there is no observable that allows for full recovery of the information encoded in the classical message.  \n",
    "<br>\n",
    "<br>\n",
    "$\\Rightarrow \\rho$ transmits less information through the quantum channel than the amount contained in the original classical message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a9f650",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Example 1:</b> Orthogonal states.  \n",
    "<br>\n",
    "<br>\n",
    "Suppose Alice has a random source of orthogonal states  \n",
    "<br>\n",
    "<br>    \n",
    "$$X = \\{ \\ket{\\psi_i}, p_i\\} = \\{ (\\ket{0}, p_0= 1/4), (\\ket{1}, p_1 = 3/4)\\}$$  \n",
    "<br>\n",
    "Bob describes the system using the density matrix  \n",
    "$$\n",
    "\\rho = p_0\\ket{0}\\bra{0} + p_1\\ket{1}\\bra{1} = \\begin{bmatrix} p_0 & 0 \\\\ 0 & p_1 \\end{bmatrix}\n",
    "$$  \n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6497c227",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "and the associated Shannon entropy will be  \n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    "S(\\rho) &=&  -\\Tr \\rho\\log \\rho = - \\Tr \\left( \\begin{bmatrix} p_0 & 0 \\\\ 0 & p_1 \\end{bmatrix}  \\begin{bmatrix} \\log p_0 & 0 \\\\ 0 & \\log p_1 \\end{bmatrix} \\right) \\nonumber\\\\\n",
    "&=& \\rule{0mm}{5mm}\n",
    "-p_0\\log p_0 - p_1 \\log p_1 = H(p_0,p_1) \\nonumber\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "Therefore, for orthogonal states, the Von Neumann and Shannon entropies are equal  \n",
    "<br>    \n",
    "<br>    \n",
    "\\begin{eqnarray}\n",
    "S(\\rho)\\rule{0mm}{8mm}&=& -\\frac{1}{4}\\log \\frac{1}{4} - \\frac{3}{4} \\log \\frac{3}{4}    = 0.81 \\, \\hbox{bits} = \n",
    "  H(X)   \\, . \\nonumber\n",
    "\\end{eqnarray}\n",
    "<br>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2f5e41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Example 2:</b> non-orthogonal states  \n",
    "<br>\n",
    "<br>    \n",
    "Now consider another source from Alice that produces a set of states with identical probabilities $p_i$  \n",
    "<br>\n",
    "<br>    \n",
    "$$\\{ \\ket{\\psi_i}, p_i\\} = \\left\\{\\rule{0mm}{4mm} (\\ket{0}, p_0= 1/4)\\, , \\, (\\ket{+},p_+ = 3/4)\\right\\}$$  \n",
    "<br>    \n",
    "Bob now writes the density matrix  \n",
    "<br>    \n",
    "<br> \n",
    "$$\n",
    "\\rho = \\frac{1}{4}\\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\end{bmatrix} + \\frac{3}{8} \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 5 & 3 \\\\ 3 & 3 \\end{bmatrix} \\,\n",
    "$$        \n",
    "<br> \n",
    "By diagonalizing, we obtain the eigenvalues  \n",
    "$$\\lambda_i = \\frac{1}{2} \\pm \\frac{1}{4} \\sqrt{\\frac{5}{2}}$$   \n",
    "<br>     \n",
    "Now we compute the Shannon entropy  \n",
    "<br>    \n",
    "$$\n",
    "S(\\rho) = -\\sum_i \\lambda_i \\log \\lambda_i \\, =\\,  0.485\\, \\hbox{bits} \\, <\\,   0.81   \\, \\hbox{bits} ~=~   H(X)\n",
    "$$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2857a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Note:</b>  \n",
    "<br><br>\n",
    "The fact that \\( S \\) is smaller than \\( H \\) also suggests that a quantum alphabet \\( X = \\{\\ket{\\psi_a}, p_a\\} \\) could admit an encoding with fewer resources than a classical one. That is, greater compression.  \n",
    "<br><br>\n",
    "The proof of this fact is given by Schumacher’s theorem.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae7d833",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Measurement Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0e461e",
   "metadata": {},
   "source": [
    "Bob receives a system in a state $\\rho$ and applies a projective measurement $\\{E_m = P_m\\}$, where  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "P_l^2 = P_l~,~~ P_m P_n = P_m\\delta_{mn}~,~~ \\sum_m P_m = I\n",
    "$$\n",
    "\n",
    "If <u>the outcome is not recorded</u>, the *non-selective measurement* has the following effect on the state:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\rho ~~ \\stackrel{\\{P_m\\}}{\\longrightarrow} ~~ \\rho' = \\sum_m P_m \\rho P_m\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddce26a3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Theorem:</b> \n",
    "<br>    \n",
    "In a non-selective projective measurement, the entropy does not decrease, \n",
    "$\n",
    "S(\\rho') \\geq S(\\rho)\n",
    "$.    \n",
    "The inequality is saturated when the basis of the projective measurement diagonalizes $\\rho = \\sum_m\\lambda_m P_m $.\n",
    "<br>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae065f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<details>\n",
    "<summary><p > >> <i>Proof</i> </p></summary>\n",
    "    \n",
    "We want to prove that \n",
    "$$\n",
    "0 ~\\leq -S(\\rho) + S(\\rho') = -S(\\rho) -\\tr (\\rho'\\log \\rho')\n",
    "$$\n",
    "    \n",
    "We know a very similar inequality, Klein's inequality, \n",
    "    \n",
    "$$\n",
    "0~\\leq ~S(\\rho\\|\\rho') ~=~ -S(\\rho) -\\tr (\\rho\\log \\rho')\n",
    "$$\n",
    "    \n",
    "It would be sufficient to prove that the second terms are equal: $\\tr (\\rho\\log \\rho') = \\tr (\\rho'\\log \\rho')$\n",
    "\n",
    "$$\n",
    "\\tr (\\rho'\\log \\rho') = \\tr\\left[ \\sum_l  P_l \\rho P_l \\log \\rho' \\right] \n",
    "$$\n",
    "Let's examine: \n",
    "\\begin{eqnarray}\n",
    "P_l \\rho' &=& P_l\\sum_m P_m \\rho P_m = \\sum_m P_l\\delta_{lm}\\rho P_m = P_l\\rho P_l \\nonumber \\\\\n",
    "\\rho'P_l  &=& \\sum_m P_m \\rho P_mP_l = \\sum_m P_m\\rho P_l \\delta_{lm} = P_l\\rho P_l\\nonumber  \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "From this we deduce that $~\\rho' P_l = P_l \\rho' ~\\Rightarrow ~\\log\\rho' P_l = P_l \\log \\rho'~$,\n",
    "and therefore\n",
    "    \n",
    "$$\n",
    "\\tr(\\rho'\\log \\rho') =  \\tr\\left[\\sum_l\\left( P_l \\rho \\log \\rho' P_l\\right)\\right]= \\tr \\left[\\left(\\sum_l P_l^2\\right) \\rho \\log \\rho'\\right] = \\tr(\\rho\\log \\rho')\n",
    "$$\n",
    "\n",
    "and thus we arrive at the desired result.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114c6957",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise:</b>  \n",
    "<br> Work in a Hilbert space $\\Hil$ of dimension 6. Randomly define a collective $\\{\\ket{\\psi_a},q_a\\},\\, a = 0,...,r-1$.  \n",
    "Perform a non-selective projective measurement in the computational basis $\\ket{i}$. Obtain the entropy variation due to the measurement.  \n",
    "\n",
    "Repeat the process, performing the measurement in the eigenbasis $\\ket{\\lambda_i}$ of $\\rho$.  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d5a6e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy of Statistical Mixtures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73a128",
   "metadata": {},
   "source": [
    "The idea is to compare the entropies of a series of states $\\rho_i\\, i=1,2,...r$ with that of a statistical mixture of mixed states\n",
    "\n",
    "$$\n",
    "\\rho = \\sum_{i=1}^r p_i \\rho_i\n",
    "$$\n",
    "with $p_i \\geq 0, \\, \\sum_{i=1}^r p_i = 1$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e241f1a0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Theorem:</b> \n",
    "<br>    \n",
    "Let $\\rho = \\sum_{i=1}^r p_i\\rho_i$ be a statistical mixture of states $\\rho_i$ with probabilities $p_i\\geq 0,\\, \\sum_{i=1}^r p_i = 1$. The following inequalities hold:\n",
    "<br> \n",
    "<br>    \n",
    "$$\\fbox{$\n",
    "~\\sum_{i=1}^r p_i S(\\rho_i) ~~\\leq~  S(\\rho) ~\\leq~ \\sum_{i=1}^r p_i S(\\rho_i) + H(\\{p_i\\}) ~\n",
    "$}\n",
    "$$   \n",
    "<br>\n",
    "<br>\n",
    "The inequality on the right is saturated when the $\\rho_i$ have support on mutually orthogonal subspaces.    \n",
    "<br>\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9097952",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p > >> <i>Proof</i> </p></summary>\n",
    "\n",
    "The inequality on the left has already been mentioned; it is the property of <b>concavity</b> of the entropy.\n",
    "Its proof is simple using the triangle inequality, which will be proven later.\n",
    "\n",
    "To prove the inequality on the right, we will start by considering the case where $\\rho_i = \\ketbra{\\psi_i}{\\psi_i}$ are pure states, **not necessarily orthogonal**.\n",
    "\n",
    "We introduce an auxiliary system $B$ with dimension $d_B \\geq r$ and orthonormal basis $\\{\\ket{i}\\}$ and define $\\rho_{AB} = \\ketbra{AB}{AB}$ in terms of the entangled state\n",
    "\n",
    "$$\n",
    "\\ket{AB} = \\sum_{i=1}^r \\sqrt{p_i} \\ket{\\psi_i}\\ket{i}\n",
    "$$\n",
    "\n",
    "Since $\\rho_{AB}$ is pure, its partial traces coincide and, therefore, their entropies are also equal:\n",
    "\n",
    "$$\n",
    "S(B) = S(A) = S\\big(\\sum_{i} p_i\\ketbra{\\psi_i}{\\psi_i}\\big) = S(\\rho)\n",
    "$$\n",
    "\n",
    "Next, we perform a projective, non-selective measurement on $B$ using the projectors $P_i^B = \\ketbra{i}{i}$\n",
    "\n",
    "$$\n",
    "\\rho_B'= \\sum_i P_i^B\\rho_B P_i^B = \\sum_i p_i \\ketbra{i}{i}\n",
    "$$\n",
    "\n",
    "We have proven that a non-selective measurement can only increase the entropy, i.e.,\n",
    "\n",
    "$$\n",
    "S(B)= S(\\rho) \\leq S(\\rho_B') = - \\sum_i p_i\\log p_i = H(\\{p_i\\})\n",
    "$$\n",
    "\n",
    "Therefore, when $\\rho_i$ are pure states, we have:\n",
    "\n",
    "$$\n",
    "S (\\rho) \\leq H(\\{p_i\\}) + \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    "\n",
    "where we added the last term, which is zero. The inequality is saturated if the $\\ket{\\psi_i}$ are orthogonal.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Now we can address the general case in which the $\\rho_i$ are mixed states. The spectral decomposition of each $\\rho_i$ is\n",
    "\n",
    "$$\n",
    "\\rho_i = \\sum_{j=1}^N \\pi^i_j \\ketbra{e^i_j}{e^i_j}\n",
    "$$\n",
    "\n",
    "where the $r$ bases $\\{\\ket{e^i_j}\\, , j=1,...,N\\}$ are, in principle, different. We can now write\n",
    "\n",
    "$$\n",
    "\\rho ~=~ \\sum_{i=1}^r \\sum_{j=1}^N p_i \\pi^i_j \\ketbra{e^i_j}{e^i_j}  ~= ~\\sum_{i,j} q_{ij}\\rho_{ij}\n",
    "$$\n",
    "\n",
    "where we have considered $\\rho$ as a mixture of pure states $\\rho_{ij}$. We can apply the result found for pure states to this case:\n",
    "<br>\n",
    "\n",
    "$$\n",
    "S(\\rho) \\leq H(\\{q_{ij}\\}) = -\\sum_{ij} p_i\\pi^i_j \\log(p_i\\pi^i_j) = -\\sum_{ij} p_i\\pi^i_j(\\log p_i + \\log \\pi^i_j)\n",
    "= -\\sum_i p_i \\log p_i -\\sum_i p_i\\big( \\sum_j \\pi^i_j \\log \\pi^i_j \\big)\n",
    "$$\n",
    "\n",
    "where we used that $\\sum_j \\pi^i_j = \\tr(\\rho_i) = 1$. We now recognize in the last two terms the functions $H(\\{p_i\\})$ and $S(\\rho_i)$, that is:\n",
    "\n",
    "$$\n",
    "S(\\rho) \\leq H(\\{p_i\\}) + \\sum_i p_i S(\\rho_i)\n",
    "$$\n",
    " \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c19b4b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The inequality on the left is the concavity property, whose content is that the entropy of a mixture is greater than that of its parts.\n",
    "\n",
    "The difference is an important quantity that *should be maximized*, as it increases the amount of information in the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d029a8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Definition:</b>  <i>(Holevo information)</i>\n",
    "<br>    \n",
    "The <i>Holevo information</i> of a state $\\rho = \\sum_i p_i \\rho_i$ is defined as the entropy increase associated with the statistical mixture \n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "\\chi = S(\\rho) - \\sum_i p_i S(\\rho_i)    \n",
    "$$\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848322c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "From the previous theorem, by subtracting the quantity $\\rho = \\sum_{i=1}^r p_i \\rho_i$ from all terms, the following inequality for the Holevo information is verified:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Corollary:</b> \n",
    "In a statistical mixture $\\rho = \\sum_{i=1}^r p_i \\rho_i$, the Holevo information is bounded as follows:\n",
    "<br>\n",
    "<br>    \n",
    "$$ \n",
    "0 \\leq \\chi(\\rho) \\leq H(\\{p_i\\}) \n",
    "$$\n",
    "<br>    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160fd67",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Exercise:</b> \n",
    "<br> Work in a Hilbert space $\\Hil$ of dimension 6. Randomly define three ensembles $\\{\\ket{\\psi_a},q_a\\}_I$ with $a = 0,...,r_a-1$ and  $I=0,1,2$. With 3 random probabilities $\\{p_i\\},\\, i=0,1,2$, consider the statistical mixture $\\rho = \\sum_i p_i \\rho_i$. Compute the Holevo information and verify the bounds.\n",
    "</div>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298058be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"ent_comp\"></a>\n",
    "<table width=\"100%\">\n",
    "    <td style=\"font-size:25px;font-family:Helvetica;text-align:left;background-color:rgba(0,0,900, 0.3);\">\n",
    "<b>Quantum Entropies of Composite Systems</b>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952f25a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After the information contained in a state, the next important quantity we wish to understand is the *degree of correlation* between two systems $A$ and $B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1677062",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Considered jointly, the isolated bipartite system $AB \\sim \\Hil_A \\otimes \\Hil_B$ \n",
    "\n",
    "recall that all accessible information for observers who can measure on $AB$ ($A$, $B$) is contained in $\\rho$, $(\\rho_A, \\rho_B)$\n",
    "\n",
    "in particular, the von Neumann entropy $S(\\rho)$ measures the Shannon uncertainty associated with a preparation via a set of projective measurements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c04e93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entanglement Entropy\n",
    "<a id='ent_entrelaz'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d39ee",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It is natural to expect that the degree of entanglement between $A$ and $B$ is reflected in the partial states $\\rho_A$ and $\\rho_B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6b0c4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Definition:</b> (<i>entanglement entropy</i>)\n",
    "<br>\n",
    "The <b>entanglement entropy</b> is\n",
    "the Von Neumann entropy of a subsystem obtained by taking the partial trace over its complement:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "S(\\rho_A) = \\Tr \\rho_A \\log \\rho_A~~~~~~\\text{with} ~~~~~\\rho_A = \\Tr_B \\rho\n",
    "$$\n",
    "<br>   \n",
    "$$\n",
    "S(\\rho_B) = \\Tr \\rho_B \\log \\rho_B~~~~~~\\text{with} ~~~~~\\rho_B = \\Tr_A \\rho\n",
    "$$  \n",
    "<br>    \n",
    "</div>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a4b5af",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Notation**: Unless otherwise stated, when dealing with composite systems, we will denote\n",
    "\n",
    "$$ \n",
    "S(AB) = S(\\rho) ~~~~~~~~~ S(A) = S(\\rho_A)  ~~~~~~~~~~ S(B) = S(\\rho_B)\n",
    "$$\n",
    "\n",
    "where it is understood that these refer to the systems obtained via partial traces.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8acacbd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Entropy of an Uncorrelated State\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0f5d48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b> Theorem</b> <i></i> \n",
    "<br>\n",
    "for an uncorrelated state we find\n",
    "$$\n",
    "S(\\rho) = S (\\rho_A\\otimes \\rho_B) = S(\\rho_A) + S(\\rho_B)\n",
    "$$\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68153f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p style=\"text-align: right ; color:black\"> >> Proof </p></summary>\n",
    "Working with the spectral decompositions of $\\rho_A$ and $\\rho_B$, we find that\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\log (\\rho_A\\otimes \\rho_B) &=& \\big( \\sum_{i,a} \\log(\\lambda_i \\mu_a) \\ketbra{\\lambda_i \\mu_a}{\\lambda_i\\mu_a}\\big) \\\\\n",
    "&=& \\big( \\sum_{i,a} (\\log\\lambda_i+ \\log \\mu_a)  \\ketbra{\\lambda_i \\mu_a}{\\lambda_i\\mu_a}\\big)  + \\\\\n",
    "&=& \\sum_{i} \\log \\lambda_i \\ketbra{\\lambda_i}{\\lambda_i}\\otimes \\sum_a \\ketbra{\\mu_a}{\\mu_a} + \n",
    "\\sum_{i}  \\ketbra{\\lambda_i}{\\lambda_i}\\otimes \\sum_a \\log \\mu_a \\ketbra{\\mu_a}{\\mu_a} \\\\\n",
    "&=& \\log\\rho_A\\otimes I + I\\otimes \\log \\rho_B\n",
    "\\end{eqnarray}  \n",
    "Then\n",
    "\\begin{eqnarray}\n",
    "S(\\rho_A\\otimes \\rho_B) &=& -\\tr_{AB} \\left[(\\rho_A\\otimes \\rho_B)\\log (\\rho_A\\otimes\\rho_B)\\right] \\\\\n",
    " &=& -\\tr_{AB} \\left[(\\rho_A\\otimes \\rho_B)(\\log\\rho_A\\otimes I + I\\otimes \\log \\rho_B)\\right] \\\\\n",
    "&=&- \\tr_{AB}\\left[\\rho_A\\log\\rho_A\\otimes \\rho_B + \\rho_A \\otimes \\rho_B \\log\\rho_B\\right]\\\\\n",
    "&=& \\tr(\\rho_A\\log\\rho_A)\\otimes \\tr \\rho_B + \\tr\\rho_A \\otimes \\tr(\\rho_B \\log\\rho_B)\\\\\n",
    "&=& S(A) + S(B)\n",
    "\\end{eqnarray}\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377b88e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "This notion of non-correlation is analogous to that which exists in classical probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc33950",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Classically, there are several entropies that play a central role: conditional entropy $H(X|Y)$, relative entropy $H(X\\|Y)$, and mutual information $I(X,Y)$.\n",
    "All three admit interpretations in terms of uncertainties and expectations.\n",
    "\n",
    "We can define *formally analogous* quantities in the quantum context, even though the probabilistic interpretation is not as evident—or may even be unknown.\n",
    "\n",
    "We have already defined [relative entropy](#entrop_rel), along with its important property of positivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8531c6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id ='infor_mutua'></a>\n",
    "## Mutual Information\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db82f71f",
   "metadata": {},
   "source": [
    "The definition of mutual information is the same, replacing Shannon entropy with von Neumann entropy  \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "$$\n",
    "I(A,B) = S(A) + S(B) - S(AB) \n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354a3499",
   "metadata": {
    "run_control": {
     "marked": true
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem</b> <i>(Positivity of Mutual Information)</i> \n",
    "<br>\n",
    "$$\n",
    "I(A,B) \\geq 0\n",
    "$$\n",
    "<br>    \n",
    "and the inequality is saturated, $I(A,B) = 0$, if and only if $\\rho = \\rho_A\\otimes \\rho_B$ is factorizable.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d5aef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "<summary><p style=\"text-align: right ; color:black\"> >> Demostración </p></summary>\n",
    "<br>\n",
    "<br>    \n",
    "Consideremos la entropía relativa asociada a los estados $\\rho  = \\rho_{AB}$ y  $\\sigma = \\rho_A\\otimes \\rho_B$\n",
    "<br>\n",
    "Entonces, por la desigualdad de Klein\n",
    "<br>    \n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    "0\\leq S(\\rho\\|\\sigma) &=& \\tr\\left(\\rho_{AB}(\\log \\rho_{AB} - \\log (\\rho_A\\otimes \\rho_B) \\rule{0mm}{4mm}\\right)\\nonumber\\\\  \\rule{0mm}{8mm}\n",
    "&=& \\tr \\left(\\rho_{AB}(\\log \\rho_{AB} - \\log (\\rho_A\\otimes I) -\\log ( I \\otimes \\rho_B) \\rule{0mm}{4mm} \\right)\n",
    "\\\\ \\rule{0mm}{8mm}\n",
    "&=& S(AB) - \\tr_{A}(\\tr_B\\rho_{AB}) - \\tr_B(\\tr_A\\rho_{AB}) \\\\ \\rule{0mm}{8mm}\n",
    "&=& S(AB) - S(A) - S(B) \n",
    "\\nonumber \\\\\n",
    "\\end{eqnarray}\n",
    "<br>    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9b90cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conditional Entropy\n",
    "\n",
    "Finally, we will copy the definition of conditional entropy, even though a notion of quantum conditional probability does not exist\n",
    "\n",
    "$$\n",
    "S(A|B) = S(AB) - S(B)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92082b3b",
   "metadata": {},
   "source": [
    "Here we find a genuine feature:\n",
    "unlike the classical case, $S(A|B)$ *can be negative*. \n",
    "\n",
    "- For example, if $AB$ is a pure state $\\Rightarrow S(AB) = 0$\n",
    "<br>\n",
    "\n",
    "- However, it cannot be *too* negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc818b3f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b>  \n",
    "<br>\n",
    "$$\n",
    "S(A|B) \\geq - \\hbox{min}(S_A, S_B)\n",
    "$$  \n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a36432c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "    <summary><p style=\"text-align: right; color:black\"> >> <i>Proof</i> </p></summary>\n",
    "<br>\n",
    "On one hand, from the fact that $S(AB)\\geq 0$ it follows that\n",
    "$$\n",
    "S(A|B) \\geq - S(B)\n",
    "$$ \n",
    "<br>    \n",
    "On the other hand, let us couple $AB$ to a third system $C$ and consider a pure state $ABC$ (i.e., $\\rho_{ABC} = \\ket{\\psi_{ABC}}\\bra{\\psi_{ABC}}$).\n",
    "    \n",
    "Then we know that $S(AB) = S(C)$ and also $S(B) = S(AC)$. We find that\n",
    "<br>    \n",
    "$$\n",
    "S(A|B) = S(AB)- S(B) = S(C) - S(AC) \\geq -S(A)\n",
    "$$\n",
    "<br>    \n",
    "where we have used the positivity of the mutual information between $A$ and $C$.  \n",
    "\n",
    "Thus the result follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d7f7e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Conditional entropy plays a fundamental role in the possibility of establishing *teleportation protocols*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c4534",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id='desig_triang'></a>\n",
    "### Araki–Lieb Triangle Inequality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3617ac16",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can now state certain relationships between the entropy of a system $AB$ and that of its parts $A$ and $B$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af613ac",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem</b> \n",
    "<br>   \n",
    "The von Neumann entropies of a composite system $AB$ and its parts $A,B$ satisfy, for any state $\\rho$, the following inequality:\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "|S(A) - S(B)| ~~ \\leq ~~ S(AB) ~~\\leq ~~S(A) + S(B)\n",
    "$$   \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "- The inequality on the left is known as the *Araki–Lieb inequality*.  \n",
    "<br>\n",
    "\n",
    "- The inequality on the right is known as the *subadditivity* of entropy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d45a2da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<details>\n",
    "    <summary><p style=\"text-align:right ; color:black\"> >> <i>Proof</i> </p></summary>\n",
    "<br>\n",
    "The inequality on the right is called <i>subadditivity</i> and it is equivalent to the positivity of relative entropy.\n",
    "<br>\n",
    "<br>\n",
    "To prove the inequality on the left (Araki–Lieb), we recall the lower bound for conditional entropy:\n",
    "<br>\n",
    "<br>\n",
    "\\begin{eqnarray}\n",
    " S(A|B) = S(AB) - S(B) &\\geq & - \\hbox{min}(S_A,S_B) \\geq -S_A \\\\\n",
    " S(B|A) = S(AB) - S(A) &\\geq& - \\hbox{min}(S_A,S_B) \\geq -S_B \\rule{0mm}{6mm}\n",
    "\\end{eqnarray}  \n",
    "<br>\n",
    "From this it follows that\n",
    "$$\n",
    "S(AB) \\geq  |S(A)- S(B)|\n",
    "$$\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce7056",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The subadditivity property allows for a simple proof of the *concavity* of the von Neumann entropy.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "    <b>Corollary:</b> The von Neumann entropy is a <b>concave</b> function of its argument\n",
    "<br>\n",
    "<br>    \n",
    "$$\n",
    "\\sum_i p_i S(\\rho_i) \\leq S\\left(\\sum_i p_i \\rho_i\\right)\n",
    "$$\n",
    "</p>\n",
    "</div>    \n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align:right ; color:black\"> >> <i>Proof</i> </p></summary>    \n",
    "\n",
    "Let $\\{\\lambda_{i,a}\\}, a=1,...,d_A$ be the eigenvalues of $\\rho_i$, then\n",
    "$$\n",
    "S(\\rho_i ) = -\\sum_{a}\\lambda_{i,a}\\log\\lambda_{i,a}\n",
    "$$\n",
    "\n",
    "Consider an auxiliary system $B$ with orthonormal basis  $\\{\\ket{i}\\}$ and density matrix\n",
    "$$\n",
    "\\rho_B = \\sum_i p_i \\ket{i}\\bra{i}\n",
    "$$\n",
    "and define the following joint $AB$ state\n",
    "$$\n",
    "\\rho_{AB} = \\sum_{i} p_i \\rho_i \\otimes \\ket{i}\\bra{i}\n",
    "$$\n",
    "If $\\rho_i$ has eigenvectors $\\ket{e_{i,a}}$ with eigenvalues $\\lambda_{i,a}$ such that $\\sum_a \\lambda_{i,a} = 1$, then the eigenvalues of $\\rho_{AB}$ are $\\{ p_i \\lambda_{i,a} \\}$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "S_{AB} &=& -\\sum_{i,a} p_i \\lambda_{i,a} \\log( p_i \\lambda_{i,a})  \\\\\n",
    "&=& \\sum_i p_i \\log p_i \\sum_a \\lambda_{i,a} + \\sum_i p_i \\sum_a \\lambda_{i,a} \\log \\lambda_{i,a} \\nonumber\\\\\n",
    "&=& \\sum_i p_i \\log p_i + \\sum_i p_i S(\\rho_i) \\nonumber\\\\\n",
    "&=& S_B + \\sum_i p_i S(\\rho_i) \n",
    "\\end{eqnarray}\n",
    "\n",
    "Taking the partial traces we find\n",
    "\\begin{eqnarray}\n",
    "\\rho_A &=& \\tr_B \\rho_{AB} =  \\sum_{i} p_i \\rho_i \\nonumber\\\\\n",
    "\\rho_B &=& \\tr_A \\rho_{AB} = \\sum_i p_i \\ket{i}\\bra{i} \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "Now, using the subadditivity property\n",
    "$$\n",
    "S_{AB} \\leq S_A + S_B\n",
    "$$\n",
    "we obtain \n",
    "$$\n",
    "S_B + \\sum_i p_i S(\\rho_i) \\leq S_A + S_B\n",
    "$$\n",
    "and canceling out $S_B$ we get\n",
    "$$\n",
    "\\sum_i p_i S(\\rho_i) \\leq S_A = S\\left(\\sum_i p_i \\rho_i\\right)\n",
    "$$\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37202c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at cases that saturate these inequalities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c86ffea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Case 1: Saturation of Subadditivity: Factorizable State ##\n",
    "\n",
    "Suppose the system $AB$ is in a *factorizable* composite state. Then the *mutual information* is zero and, consequently, the *subadditivity* is saturated:\n",
    "\n",
    "$$\n",
    "\\rho_{AB} = \\rho_A \\otimes \\rho_B ~~~~\\Leftrightarrow ~~~~ S(AB) = S(A) + S(B)\n",
    "$$\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cab1e4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Case 2: Saturation of Araki–Lieb: Pure or Entangled State\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836fe84",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We begin by writing the pure state\n",
    "\n",
    "$$\n",
    "\\rho_{AB} = \\ket{\\psi_{AB}} \\bra{\\psi_{AB}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069f2a1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In a pure state of a bipartite system $AB$, entanglement introduces quantum correlations between the two systems\n",
    "\n",
    "$$\n",
    "\\ket{\\psi_{AB}} = \\sum_{i,j} c_{ij} \\ket{i}_A \\otimes \\ket{j}_B\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2956340",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One way to determine if entanglement exists is to use the Schmidt decomposition.\n",
    "\n",
    "$$\n",
    "\\ket{\\psi_{AB}} = \\sum_{a=1}^r \\sqrt{p_a}\\,  \\ket{\\psi^a_A} \\otimes \\ket{\\psi^a_B}\n",
    "$$\n",
    "\n",
    "If and only if the Schmidt number, $r$, is greater than one, the state is entangled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19af94e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we want to be more precise and propose a way to quantify the extent of such correlations. This is what the *Entanglement Entropy* measures.\n",
    "\n",
    "Indeed,\n",
    "\n",
    "$$\n",
    "S(\\rho_A) = S(\\rho_B) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n",
    "\n",
    "Therefore, the *entanglement entropy* is proportional to the *entanglement* present in $\\ket{\\psi_{AB}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71ff8f5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "<b>Theorem:</b> Let ${AB}$ be a composite system in a pure state with $ S(AB) = 0$. The entanglement entropy of its constituent subsystems $A$ and $B$ is  \n",
    "<br>\n",
    "<br>    \n",
    "$\\to ~$ equal for both subsystems $S(A) = S(B)$\n",
    "<br>\n",
    "<br>    \n",
    "$\\to ~$ proportional to the entanglement of the pure state $\\ket{\\psi_{AB}}$\n",
    "</p>\n",
    "</div>    \n",
    "\n",
    "<details>\n",
    "<summary><p style=\"text-align: right ; color:black\"> >> Proof </p></summary>\n",
    "<br>\n",
    "The density matrix $\\rho_{AB} = \\ket{\\psi_{AB}}\\bra{\\psi_{AB}}$ has zero entropy $S(\\rho_{AB})=0$. This is not the case for the density matrices of the subsystems $A$ and $B$.\n",
    "Writing $\\ket{\\psi_{AB}}$ in the Schmidt basis, we can compute\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "\\rho_{A} = \\Tr_B \\rho_{AB} = \\sum_{a} p_a \\ket{\\psi^a_A}\\bra{\\psi^a_A}\n",
    "~~~~~~~,~~~~~~~~\n",
    "\\rho_{B} = \\Tr_A \\rho_{AB} = \\sum_{a} p_a \\ket{\\psi^a_B}\\bra{\\psi^a_B}\n",
    "$$\n",
    "Since the $\\ket{\\psi^a}$ are orthonormal, for the entropies of the subsystems we find\n",
    "$$\n",
    "S(A) = S(B) = - \\sum_{a=1}^r p_a \\log p_a\n",
    "$$\n",
    "<br>\n",
    " - they are proportional to the degree of entanglement of $\\ket{\\psi_{AB}}$. \n",
    "    \n",
    "Indeed, if $p_1= 1$ and $p_{i>1} = 0$, so that there is no entanglement, then the entropies satisfy $S(A) = S(B) = 0$. \n",
    "    \n",
    "On the other hand, if the state is maximally mixed $S = \\log N$, then the entanglement is also maximal with $p_a = \\frac{1}{N}$ for $a = 1,..., N$.\n",
    "<br>\n",
    "<br>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4156ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We see that, in this case, the Araki–Lieb inequality is saturated:\n",
    "\n",
    "$$\n",
    "|S(A) - S(B)| = 0 = S(AB)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfc1b30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<a id=\"codif_optim\"></a>\n",
    "<table width=\"100%\">\n",
    "    <td style=\"font-size:25px;font-family:Helvetica;text-align:left;background-color:rgba(0,0,900, 0.3);\">\n",
    "<b>Quantum Coding</b>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca15b71",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\",text-align:center>\n",
    "<p style=\"text-align: left ;color: navy;\">  \n",
    "    <b>Theorem:</b> <i>Schumacher's theorem</i>\n",
    "    <br>\n",
    "Given a message whose letters are pure states drawn independently from the quantum alphabet $X=  \\{ \\ket{\\psi_i}, p_i\\}$,  there exists \n",
    "    an <i>optimal lossless coding</i> that makes an average use of $S(\\rho)$ <i>qubits per letter</i>, where $\\rho = \\sum_i p_i \\ketbra{\\psi_i}{\\psi_i}$.\n",
    "</p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f254598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
